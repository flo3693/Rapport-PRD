\documentclass[overfullbox, poster]{polytech/polytech}
\let\oldtabular=\tabular
\def\tabular{\scriptsize\oldtabular}


% LISEZ LE MODE D'EMPLOI POUR L'INTEGRALITE DES CONSIGNES

% on ajoute ici des packages supplémentaires
% Attention : il peut y avoir des incompatibilités avec la classe de document

\usepackage{lipsum} %permet de générer du texte

% on indique le département concerné (di)
\schooldepartment{di}
% on indique le type de projet concerné (prd)
\typereport{prd}
% on indique l'année en cours
\reportyear{2015-2016}

% on donne un titre au travail (sans dépasser 2 lignes à l'affichage)
\title{Application d'aide à l'interaction homme/machine pour les personnes handicapées}
% on peut donner un sous titre (sans dépasser 2 lignes à l'affichage) mais ce n'est pas nécessaire
%\subtitle{Il peut y avoir un sous titre mais ce n'est pas obligatoire}
% on peut donner un logo illustrant le projet (hauteur d'affichage 4cm) mais ce n'est pas nécessaire
%\reportlogo{polytech/modeemploi}

% on indique les contributeurs à ce rapport

% Le(s) étudiant(s) aux formats suivants :
% \student{Prénom}{Nom}{Mail}
% \student[Année d'études]{Prénom}{Nom}{Mail}
\student[DI5]{Florian}{Tissier}{florian.tissier@etu.univ-tours.fr}
% Le(s) superviseur(s) académique (ou encadrant(s)) aux format suivants :
% \academicsupervisor{Prénom}{Nom}{mail}
% \academicsupervisor[Affiliation]{Prénom}{Nom}{mail}
\academicsupervisor[Département infomatique]{Mohamed}{Slimane}{mohamed.slimane@univ-tours.fr}
\academicsupervisor[Département infomatique]{Donatello}{Conte}{donetello.conte@univ-tours.fr}
% Le(s) tuteur(s) entreprise aux formats suivants :
% \companysupervisor{Prénom}{Nom}{Mail}
% \companysupervisor[Fonction]{Prénom}{Nom}{Mail}
% L'entreprise aux formats suivants :
%	\company{Nom de l'entreprise}{Adresse}{URL du site web}
% \company[logo entreprise]{Nom de l'entreprise}{Adresse}{URL du site web}
% S'il est indiqué le logo de l'entreprise s'affichera sur une hauteur de 1cm
% Attention : pour que les tuteurs entreprise s'affichent, l'entreprise doit être définie
%\company[polytech/polytech]{Laboratoire Informatique}{64 avenue Jean Portalis\\37200 Tours}{http://li.univ-tours.fr}
%\company{Laboratoire Informatique}{64 avenue Jean Portalis, 37200 Tours}{li.univ-tours.fr}


% On indique les mots clés avec \motcle{mot clé} en français et \keyword{keyword} en anglais
% Le résumé significatif et descriptif du contenu du rapport en 5 à 10 lignes se spécifie par \resume{...} en français et \abstract{...} en anglais
% Attention : tout doit tenir sur la dernière page
\resume{Résumé}
% chaque mot clé ou groupe de mots clés est défini via la commande \motcle en français
\motcle{mot}
\motcle{clé}
\motcle{deux mots}

% résumé en anglais
\abstract{Abstract}

% chaque mot clé ou groupe de mots clés est défini via la commande \motcle en anglais
% Attention : il n'y a pas forcément une traduction directe entre les mots clés et les keywords
\keyword{word}
\keyword{key}
\keyword{two words}
\keyword{fourth word}



\bibliography{biblio}

\posterblock{Objectif}{
L'objectif de ce projet est de réaliser une application qui, à l'aide d'une caméra, va repérer le visage d'une personne handicapée puis zoomer dessus pour ensuite analyser son émotion actuelle.\\
En fonction de l'émotion perçue, une action spécifique sera réalisée (ex: si la personne est triste, lui mettre de la musique joyeuse). 
}{images/ptz.png}{}
\posterblock{Normes de description}{
	%Plusieurs normes permettent de définir des expressions du visage et donc des émotions.\\
%Une des plus connue est la norme \textbf{FACS}, qui signifie Facial Action Coding System. Ce système décompose tous les mouvements du visage en 46 \textbf{Action Units} (AU), chacune décrivant la contraction ou la décontraction d'un ou plusieurs muscles du visage.  Par exemple un sourire, et donc l'émotion de la joie, est composé des AUs 6 (remontée des joues) et 12 (étirement du coin des lèvres).\\
%Plusieurs niveaux d'intensités sont également défini, allant de A (très faible) à E (maximum), pour chaque AU.
}{images/AU.png}{Exemple d'AUs utilisées dans le système FACS}
\posterblock{Ajouts fonctionnels}{
	%La réalisation d'une telle application passe par une phase d'apprentissage. Dans cette phase, l'application va apprendre à reconnaître telle ou telle émotion en utilisant des images déjà labellisées avec le nom d'une émotion ou avec une des normes de description comme FACS. Ces images se trouvent dans différentes bases de données de visages comme la base Cohn-Kanade ou la base HCI-Tagging.
}{images/ck.jpg}{Exemple d'images (issues de Cohn-Kanade) utilisées pour l'apprentissage}

% le document commence ici
\begin{document}

% on commence par générer la page de titre, la liste des intervenants, les tables des matières, figures, tables, listings
\maketitle


% Le chapitre d'introduction générale n'est souvent pas numéroté
% La commande \unnumberedchapter fonctionne de façon identique à \chapter mais produit un chapitre non numéroté présent dans la table des matières (à la différence de \chapter*)
% Attention : un chapitre sans numéro (et ses sections) ne peu(ven)t pas être référencé(s) dans le document (\label, \ref)
\unnumberedchapter[Introduction]{Introduction}
L'interaction entre les hommes et les machines a toujours été un enjeu de taille. Arriver à faire communiquer un ordinateur avec un être humain est un défi de tous les jours et est de plus en plus présent dans notre quotidien. Nous pouvons par exemple cité \textit{Siri} d'Apple qui permet de communiquer avec son smartphone simplement en parlant.\\ 
C'est dans cet optique de facilitation du quotidien grâce à l'interaction avec une machine que ce projet prend place.\\
\\
Le Projet de Recherche et Développement (anciennement Projet de Fin d'Études) est un projet se déroulant durant toute la 5ème année de master ingénieur au sein de l'école Polytech Tours. Ce rapport va présenter les travaux que j'ai effectué durant toute la durée de ce projet.\\
\\
Ce rapport va donc se diviser en deux grandes parties: tout d'abord la partie Recherche qui va contenir le cahier des charges du projet, l'état de l'art en matière de reconnaissance faciale d'émotions ainsi que toutes les bases théoriques dont j'aurais besoin par la suite. La deuxième partie est la partie Développement qui va se concentrer sur les différentes étapes du développement de cette application d'aide aux personnes handicapées.\\
\\
Dans la partie Recherche de ce rapport, après avoir défini le cahier des charges, je vais vous présenter les sept émotions universelles ainsi que deux normes majeures permettant de définir les mouvements du visages qui composent une expression.\\
Je vous présenterai ensuite un état de l'art en matière de reconnaissance faciale d'expression, notamment les différentes techniques permettant de capturer un visage et de reconnaître une émotion, tout d'abord en 3D puis en 2D, ainsi que les avantages et inconvénients de chacune de ces techniques. Dans chacune de ces étapes, je présenterai l'état de l'art actuelle en terme de matériel, de méthodes et également de base de données disponibles. \\
Je continuerai ensuite en présentant les différentes étapes nécessaires à la construction d'un système de reconnaissance faciale d'émotions performant et efficace.\\
Je définirai ensuite les spécifications de l'application ainsi que les choix qui ont permit cette définition.\\
Je conclurai par un planning prévisionnel ainsi que par les méthodologies et les outils de suivi utilisés durant ce projet.\\
\\
Dans la partie Développement, [rédaction en deuxième partie de l'année]\\
\\
\\
Pour ce projet de recherche et développement, j'ai été encadré par Donatello Conte et Mohamed Slimane.


\part{Recherche}
\label{part:part_recherche}

\chapter{Cahier des charges du projet}
\label{chap:chap_cdc}

\section{MOA}
\begin{itemize}
\item Donatello CONTE: Maître de conférence et enseignant chercheur en informatique à l'école Polytech Tours
\item Mohamed SLIMANE: Professeur des Universités et enseignant chercheur en informatique à l'école Polytech Tours
\end{itemize}

\section{MOE}
Florian TISSIER: élève en dernière année de master ingénieur en informatique à Polytech Tours

\section{Contexte et présentation}
Ce projet de recherche et développement prend place dans le cursus de dernière année de master ingénieur dispensé à l'école Polytech Tours.\\
\\
Monsieur Slimane étant membre d'une association s'occupant de personnes handicapées, il souhaitait pouvoir aider et faciliter la vie de ces derniers via un projet réalisé au sein de l'école.\\
Le but de ce projet de recherche et développement est de construire un système permettant, à partir d'un flux vidéo acquis grâce à une caméra, de détecter l'expression du visage actuelle d'une personne handicapée dans le but de réaliser certaines actions pouvant améliorer son bien être.

\section{Problématique et objectif}
Comment faciliter la vie quotidienne des personnes handicapées grâce à leurs émotions?\\
\\
L'objectif est de réaliser une application qui pourra détecter en temps réel les émotions d'une personne handicapée se trouvant dans une pièce à l'aide d'une caméra fixée à un mur de cette même pièce. La caméra devra être capable de bouger et de zoomer pour suivre le visage de la personne.\\
Une fois l'émotion détecté, des actions spécifiques devront être réalisées (ex: changement de la couleur de la lumière, lecture de musique douce...)

\section{Périmètre}
Ce projet se concentre principalement sur les personnes handicapées mais l'application qui résultera de ce projet pourra également être utilisé pour des personnes non handicapées.\\
L'application devra être fonctionnel dans n'importe quelle pièce d'une maison ou d'une structure spécialisée dans l'accueil de personnes handicapées.

\section{Description fonctionnelle}
Le projet se découpe en 2 fonctions principales:
\begin{itemize}
\item Repérer le visage
\item Reconnaître l'expression\\
\end{itemize}

Chacune de ces fonctions se décomposent en plusieurs sous-fonctions.

\subsection{Repérer le visage}
Cette première focntion principale va permettre de trouver le visage d'une personne, de le suivre et de zoomer dessus.\\
Les sous-fonctions suivantes seront donc nécessaires:
\begin{itemize}
\item Un algorithme de détection de visage dans une image
\item Un algorithme de suivi de visage
\item Un algorithme de zoom\\
\end{itemize}

Vous trouverez ci-après un descriptif plus précis de chacune de ces sous-fonctions.

\begin{tabular}{|l|p{12.5cm}|}
   \hline
   \multicolumn{2}{|l|}{\textbf{Fonction: Repérer le visage/Détection du visage}} \\
   \hline
   Objectif &Détecter un visage dans un environnement quelconque.\\
   \hline
   Description &En analysant les frames d'un flux vidéo, cet algorithme nous renverra la position d'un cadre entourant le visage trouvé.\\
   \hline
   Contraintes &Cet algorithme doit être rapide. \\
   \hline
   Niveau de priorité &Haute \\
   \hline
\end{tabular}

\begin{tabular}{|l|p{12.5cm}|}
   \hline
   \multicolumn{2}{|l|}{\textbf{Fonction: Repérer le visage/Suivi du visage}} \\
   \hline
   Objectif &Suivre le visage entre plusieurs frames d'un flux vidéo dans le but de ne pas le perdre. \\
   \hline
   Description &En comparant 2 frames consécutives d'un flux vidéo, l'algorithme devra nous dire le déplacement du visage pour pouvoir le suivre avec la caméra. Il devra également être capable de faire le suivi si jamais le visage se retrouve occulté pendant quelques secondes. \\
   \hline
   Contraintes &Éviter le plus possible l'accumulation d'erreur de précision pouvant amener à la perte du visage. \\
   \hline
   Niveau de priorité &Haute \\
   \hline
\end{tabular}

\begin{tabular}{|l|p{12.5cm}|}
   \hline
   \multicolumn{2}{|l|}{\textbf{Fonction: Repérer le visage/Zoom}} \\
   \hline
   Objectif &Zoomer sur une zone de l'image. \\
   \hline
   Description &L'algorithme devra pouvoir zoomer sur le cadre contenant le visage renvoyé par la sous-fonction de détection du visage. \\
   \hline
   Contraintes &Disposer d'une caméra ayant la possibilité de zoomer. \\
   \hline
   Niveau de priorité &Moyenne \\
   \hline
\end{tabular}

\subsection{Reconnaître l'expression}
Cette deuxième fonction principale va permettre d'identifier l'expression faciale de la personne.\\
Les sous-fonctions suivantes seront donc nécessaires:
\begin{itemize}
\item Un algorithme d'apprentissage
\item Un algorithme d'extraction des points clés (\textit{features}) du visage
\item Un algorithme de classification\\
\end{itemize}

Vous trouverez ci-après un descriptif plus précis de chacune de ces sous-fonctions.

\begin{tabular}{|l|p{12.5cm}|}
   \hline
   \multicolumn{2}{|l|}{\textbf{Fonction: Reconnaître l'expression/Apprentissage}} \\
   \hline
   Objectif &Apprendre au système à classifier les expressions en fonction d'une base d'apprentissage.\\
   \hline
   Description &Pour chaque élément dans la base d'apprentissage, une émotion lui sera associé. Cela va permettre au système d'apprendre à quel expression du visage appartient une émotion.\\
   \hline
   Contraintes &La base d'apprentissage doit être assez fourni et pertinente pour permettre un apprentissage performant. \\
   \hline
   Niveau de priorité &Haute \\
   \hline
\end{tabular}

\begin{tabular}{|l|p{12.5cm}|}
   \hline
   \multicolumn{2}{|l|}{\textbf{Fonction: Reconnaître l'expression/Extraction des \textit{features} du visage.}} \\
   \hline
   Objectif &Extraire les \textit{features} du visage pour pouvoir ensuite réaliser une classification. \\
   \hline
   Description &Cet algorithme devra extraire les \textit{features} du visage d'une personne se trouvant sur une frame d'un flux vidéo. Les \textit{features} d'un visage sont par exemple le coin des yeux, la position de la pupille, les coins de la bouche, le nez, les joues... Les positions des \textit{features} retournées permettront la classification. \\
   \hline
   Contraintes & \\
   \hline
   Niveau de priorité &Haute \\
   \hline
\end{tabular}

\begin{tabular}{|l|p{12.5cm}|}
   \hline
   \multicolumn{2}{|l|}{\textbf{Fonction: Reconnaître l'expression/Classification}} \\
   \hline
   Objectif &Classifier l'expression du visage et retourner l'émotion associée. \\
   \hline
   Description &L'algorithme devra classifier l'expression du visage de la personne récupéré depuis un flux vidéo, grâce aux positions des \textit{features}, en fonction de l'apprentissage qui aura été effectué précédemment. \\
   \hline
   Contraintes &Cet algorithme doit être rapide et fiable (au moins 90\% de reconnaissance). \\
   \hline
   Niveau de priorité &Haute \\
   \hline
\end{tabular}

\section{Budget}
Ce projet ne dispose pas d'un budget précis.\\
Néanmoins en réalisant un état de l'art des matériels disponibles à notre projet, nous avons décidé de choisir une caméra ne dépassant pas les 500€.

\section{Délai}
La partie recherche devra être fini pour le 15 janvier 2016 et la partie développement (et donc le projet complet) devra être fini pour la fin du second semestre, c'est-à-dire fin mars/début avril.

\chapter{Émotions universelles et mouvements du visage}
\label{chap:chap_emotions}

Ce chapitre va me permettre d'introduire les notions nécessaires à la décomposition et donc à la reconnaissance d'une expression faciale et de son émotion associée.\\
Pour un simple sourire, nous utilisons une vingtaine de muscles (les muscles zygomatiques), il ne peut donc pas être décrit par un seul mouvement du visage mais plusieurs. C'est exactement la même chose pour une expression, on ne la reconnaît que grâce à l'ensemble des mouvements faciaux qui la composent.\\
C'est sur ce principe qu'on était crée les deux normes de description des mouvements du visage que je vais vous présenter: le FACS et MPEG-4.\\
L'utilisation de l'une ou l'autre de ces normes permet de définir entièrement le spectre des mouvements rentrant en jeu dans n'importe quelle émotion.\\
\\
Mais tout d'abord, je vais vous introduire les sept émotions universelles qui seront utilisées tout au long de mon projet et donc de ce rapport.

\newpage
\section{Les 7 émotions universelles}
\label{sec:expr_uni}

A ce jour, il a été démontré qu'il existait 7 émotions qui partagent une expression universellement compréhensible.\\
On considère qu'une émotion possède une expression universelle si tout individu est capable d'exprimer cette émotion et est également capable de la reconnaître et de l'interpréter chez autrui.\\
Les sept émotions universelles sont donc les suivantes:
\begin{itemize}
	\item la neutralité
	\item la joie
	\item la tristesse
	\item la colère
	\item la peur
	\item la surprise
	\item le dégoût
\end{itemize}

C'est Charles Darwin qui, en 1872 dans son livre \cite{darwin}, a introduit cette idée d'émotions universelles entre les hommes mais également entre différentes espèces. Il a observé que les hommes et les animaux partagent des émotions comprises par tous et qui sont nécessaires à leur survie.\\
\\
Mais ce n'est qu'en 1971 que le psychologue Paul Ekman, après un voyage en Papouasie-Nouvelle-Guinée, a confirmé les théories de Darwin. Dans son article \cite{ekman} écrit avec la participation de Wallace Fielsen, il définit les 7 émotions universelles citées plus haut.\\

\section{Le FACS}
\label{sec:facs}

En 1978, Ekman et Fielsen publie \cite{ekman2} et apporte une nouvelle pierre à l'édifice en définissant un système de codification manuelle des expressions du visage: le \textbf{Facial Action Coding System} (FACS).\\
Ce système décompose tous les mouvements du visage en 46 \textbf{Action Units} (AU), chacune décrivant la contraction ou la décontraction d'un ou plusieurs muscles du visage. La \autoref{ex_au} représente certaines de ces AUs.\\

\begin{Figure}{ex_au}{Exemples d'AUs}
 \pgfimage{images/AU.png}
\end{Figure}

La composition de plusieurs AU permet donc de décrire une expression et donc de reconnaître une émotion. Par exemple, un sourire et donc l'émotion de la joie est composé des AUs 6 (remontée des joues) et 12 (étirement du coin des lèvres).\\
La tristesse quand à elle va être composée des AUs 1, 4 et 15 et la colère des AUs 4, 5, 7 et 23.\\
N'importe quelle expression du visage peut donc être représentée par une combinaison d'AU, ce qui fait de FACS le système le plus utilisé par les psychologues ainsi que par les personnes travaillant sur la reconnaissance faciale d'émotions.\\
\\
Le système \textbf{FACS} possède également un degré d'intensité allant de A à E et permettant de spécifier l'intensité d'une AU :
\begin{itemize}
	\item A : Très Faible
	\item B : Minime
	\item C : Moyen
	\item D : Sévère
	\item E : Maximum
\end{itemize}
La surprise peut donc être défini comme la combinaison des AUs 1, 2, 5B et 26.\\
\\
Enfin des ajouts ont été apportés à cette norme. 13 nouveaux AUs ont été ajoutées pour décrire le mouvement de la tête et 7 autres pour le mouvement des yeux.\\
Nous arrivons donc a un total de 66 AUs, chacune possédant 5 intensités, permettant de décrire les mouvements faciaux.\\
\\
Néanmoins, un autre système de codification fait concurrence au FACS et est également bien implanté dans le milieu de la reconnaissance d'émotions.

\section{La norme MPEG-4}
\label{sec:fapu}

La norme MPEG-4, qui est une norme de codage vidéo, dispose de son propre système permettant de normaliser les mouvements du visage et de reconnaître des expressions.\\
Pour cela, ce système défini des points clés du visage (\autoref{points_fapu}) appelés \textbf{Facial Features Points} (FFP) auxquels seront appliqués des mesures pour créer des distances entre ces FFP (\autoref{distance_fapu}) appelées \textbf{Facial Animation Parameter Units} (FAPU).\\
Ces FAPU vont servir à la description des mouvements musculaires appelés \textbf{Facial Action Parameters} (FAP, équivalent des AUs de la norme FACS). 68 FAPs sont recensés à ce jour, j'en ai regroupé quelques uns dans le tableau \autoref{tab:fap}.\\
Le descriptif complet de ces FAP se trouve dans le document à cette adresse \cite{urlfaps}, dans l'annexe numéro 1.

\begin{Table}{tab:fap}{Exemple de Facial Action Parameters}
	\begin{tabu}{|c|c|X[m]|}
		\hline
		Numéro &Nom &Description\\\hline
		3 &open\_ jaw &Vertical jaw displacement (does not affect mouth opening)\\\hline
		7 &stretch\_ r\_ cornerlip &Horizontal displacement of right inner lip corner\\\hline
		10 &raise\_ b\_ lip\_ lm  &Vertical displacement of midpoint between left corner and middle of bottom inner lip\\\hline
		42 &lift\_ r\_ cheek &Vertical displacement of right cheek\\\hline
	\end{tabu}
\end{Table}

\begin{Figure}{points_fapu}{Quelques Facial Features Points utilisés par la norme MPEG-4}
 \pgfimage{images/points_fapu.png}
\end{Figure}

\begin{Figure}{distance_fapu}{Facial Animation Parameter Units utilisés par la norme MPEG-4}
 \pgfimage{images/distance_fapu.png}
\end{Figure}

Cependant, la norme de MPEG-4 est moins réaliste que FACS d'un point de vue musculaire.\\
Par exemple : l'AU 26 de FACS (« Jaw Drop ») décrit le mouvement d'abaissement du menton, cet abaissement est accompagné d'un abaissement de la lèvre inférieure. Or l'abaissement du menton de MPEG-4 (FAP 3 - open\_ jaw) ne décrit pas l'abaissement de la lèvre inférieure.\\
MPEG-4 ne décrit que les mouvements \textit{visibles} du visages, contrairement à FACS qui lui décrit les mouvements \textit{réalistes} du visage.\\
\\
Nous allons maintenant voir quelles technologies sont disponibles pour réaliser l'acquisition des images qui seront à traiter par la suite.

\chapter{État de l'art}
\label{chap:chap_sota}

Dans ce chapitre, je vais vous présenter l'état de l'art des systèmes et techniques existants permettant de faire de la reconnaissance facial d'émotion.\\
Mon projet se basant sur des images et vidéos en 2D, ce chapitre va principalement se concentrer sur les techniques d'acquisition et d'analyse d'images 2D.\\
Cependant, j'ai tout de même réalisé quelques recherches sur le matériel utilisé pour l'acquisition d'images en 3D ainsi que les bases de données disponibles et c'est donc par ces recherches que je vais entamer ce chapitre.

\newpage
\section{3D}
\label{sec:3d}

Comme je l'ai expliqué plus haut, mon projet et donc mon système de reconnaissance utilisera des images 2D mais j'ai tout de même mené quelques recherches sur les images en 3D également.\\
Je vais tout d'abord vous présenter les techniques utilisées pour acquérir des images en 3D. Je vous présenterai ensuite certains matériels déjà existant permettant de faire de la capture d'images 3D et utilisant les techniques que je vous aurais présenter précédemment. Je terminerai enfin cette section sur la 3D en vous présentant les bases de données de visages les plus connues et les plus utilisés par les systèmes de reconnaissance faciale d'émotion en 3D.\\
Les informations que vous allez trouver dans cette section proviennent en majorité de l'article \cite{sota3d}.

\subsection{Techniques d'acquisition}
\label{subsec:tech}

\subsubsection{Reconstruction à partir d'une image}
\label{single_img_reconstruct}
Il est possible, à partir d'une image en 2D capturé par une caméra basique, d'obtenir une image en 3D.\\
La méthode la plus prometteuse est celle du \textbf{3D Morphable Model} (3DMM), qui consiste à apposer un visage en 3D (masque) sur l'image en 2D et que le modifier pour le faire correspondre avec l'image. Sont ensuite extraites les informations correspondant au masque modifié qui vont permettre de créer le visage de l'image en 3D.\\
La \autoref{3dmm} présente un exemple de visage 3D récupéré depuis une image 2D.\\
Cette technique est très pratique et répandue car elle ne nécessite pas de matériel au coût exorbitant, une simple caméra est nécessaire.

\begin{Figure}{3dmm}{Reconstruction d'un visage 2D (1) en 3D (2) grâce à la méthode 3DMM}
 \pgfimage{images/3dmm.png}
\end{Figure}

\subsubsection{Lumière structurée}
\label{struc_light}
Une autre technique, une des plus utilisé, est la technique de la lumière structurée.\\
Elle consiste à projeter plusieurs rais de lumière (visible ou infra-rouge) de longueur d'onde différente puis, à l'aide d'un capteur, de mesurer la déformation de ces rais de lumière pour construire le visage en 3D.\\
La \autoref{structured} montre un exemple de lumière structurée.\\
L'utilisation de cette technique requiert d'avoir un matériel spécifique contenant un émetteur et un récepteur, cet outil peut aller de quelques centaines d'euros pour les moins chère à plusieurs dizaines de milliers d'euros pour les plus performants.

\begin{Figure}{structured}{Exemple de lumière structurée}
 \pgfimage{images/structured.jpg}
\end{Figure}

\subsubsection{Stéréo photométrique}
\label{photo_stereo}
La technique de la stéréo photométrique consiste à prendre plusieurs photos d'un même objet avec un même appareil sous différentes illuminations (lumière venant de droite, lumière venant de devant ...). Pour obtenir un visage en 3D, il ne reste qu'à assembler les photos obtenues.\\
La \autoref{stereo_imgs} montre un exemple de reconstruction d'un objet grâce à la stéréo photométrique.\\
Cette technique requiert également un équipement coûteux et n'est donc pas accessible à tout le monde.

\begin{Figure}{stereo_imgs}{Exemple de reconstruction d'un objet grâce à la stéréo photométrique}
 \pgfimage{images/stereo_imgs.png}
\end{Figure}


\subsubsection{Stéréo multi-vue}
\label{multi_stereo}
C'est une technique très similaire à celle de la stéréo photométrique sauf qu'au lieu de prendre en photo un visage sous différentes illuminations, le visage est pris sous différents angles simultanément avec plusieurs appareils.\\
Cette technique requiert elle aussi un équipement spécifique coûteux.\\
\\
\\

\subsection{Dispositifs d'acquisition d'images}
\label{dispositif_acquisiton}
Plusieurs types de dispositifs différents existent à l'heure actuelle permettant de capturer des images en 3D. La plupart d'entre eux se basent sur les techniques présentées précédemment.\\
Je vais ici vous présenter les deux dispositifs les plus connus.

\subsubsection{Kinect}
\label{kinect}
Probablement la caméra 3D la plus connu du grand public : la Kinect de Microsoft.\\
Créer initialement pour créer une immersion plus poussée pour les jeux de la console XBox 360, elle a depuis été amélioré et ne sert plus exclusivement qu'à jouer aux jeux vidéos.\\
Elle utilise la technique de la lumière structuré (infra-rouge dans ce cas) pour capturer les images en 3D de ce qu'elle filme.\\
La Kinect reste cependant une caméra 3D low-cost, de mauvaise qualité lorsqu'on le compare à d'autres dispositifs du même type, tel que celui que je vais présenter maintenant.

\begin{Figure}{kinect2}{Version 2 de la Kinect de Microsoft}
 \pgfimage{images/kinect2.png}
\end{Figure}

\subsubsection{Minolta Vivid 910}
\label{minolta}
Un autre dispositif très utilisé et utilisant lui aussi la technologie de la lumière structurée est le Minolta Vivid 910.\\
Un comparatif entre la qualité de Kinect et de Minolta est présenté en \autoref{kinectvsminolta}. On s'aperçoit très clairement du gouffre séparant ces deux dispositifs. Bien sûr le prix n'est pas le même, car nous passons de quelques centaines d'euros pour la Kinect à plusieurs dizaines de milliers d'euros pour le Minolta Vivid 910.

\begin{Figure}{img_minolta}{Minolta Vivid 910}
 \pgfimage{images/minolta.jpg}
\end{Figure}

\begin{Figure}{kinectvsminolta}{Comparatif de la qualité entre Kinect et Minolta}
 \pgfimage{images/kinectvsminolta.png}
\end{Figure}

\newpage
\subsection{Base de données de visages 3D}
\label{bdd3d}
Les dispositifs tels que le Minolta Vivid 910 présentées précédemment permettent également la création de bases de données de visage en 3D. Leurs grandes qualités permettent d'obtenir des images très précises, facilement exploitable.\\
Je vais maintenant vous présenter les bases de données les plus connues et pouvant être utilisée par la communauté scientifique.

\subsubsection{BU-3DFE}
Les premiers efforts pour récolter des données en 3D ont menés à la création de la base de données BU-3DFE (Binghamton University 3D Facial Expression \cite{bu3dfe_article}).\\
Les images contenues dans cette base sont statiques et ont été capturées par le dispositif 3dMD.\\
Elles se composent de 100 sujets âgés de 18 à 70 ans, dont 56\% de femmes, et appartenant à différentes ethnicités.\\
Chaque sujet réalise les 7 expressions basiques (cf \autoref{sec:expr_uni}) et chaque expression, sauf l'expression neutre, est réalisée suivant 4 niveaux d'intensités. Pour chaque sujet, il y a donc 25 images différentes, ce qui nous donne au total 2500 images dans cette base de données.\\
Un exemple de données contenues dans cette base sont présentés en \autoref{bu3dfe}.\\
Chaque donnée contient également la position de 83 points clés du visage.

\begin{Figure}{bu3dfe}{Exemple de données contenues dans BU-3DFE}
 \pgfimage{images/bu3dfe.png}
\end{Figure}

\subsubsection{BU-4DFE}
Une extension de la base BU-3DFE a été réalisé dans le but d'obtenir un espace 3D dynamique, c'est-à-dire rajouter la dimension du temps dans les images de la base, en plus des 3 dimensions déjà présentes.\\
En effet, dans la version de base, il n'y avait qu'une seule image présente pour une expression et une intensité, mais dans le but d'obtenir des analyses plus performantes, inclure la notion du temps dans ces images devient indispensable.\\
La base de données BU-4DFE \cite{bu4dfe_article} se compose donc de 101 sujets (58 femmes) de différentes ethnicités, chaque sujet réalisant 6 des expressions basiques (toutes sauf la neutre). Chaque séquence d'émotion contient environ 100 frames, ce qui nous permet d'obtenir environ 60600 différents frames dans la base.

\subsubsection{Bosphorus 3D Face Database}
La base de données Bosphorus \cite{bosphorus_article}
Cette base de données est composé de 105 sujets (45 femmes) dont la plupart sont de type Caucasien et dont un tiers sont des acteurs professionnels.\\
Chaque sujet réalise environ 35 expressions et toutes les images sont codés en terme de FACS (\autoref{sec:facs}).\\
Plusieurs illuminations et occlusions du visage (barbe, moustache, lunettes...) sont également présentes pour chaque sujet. 24 points clés du visage sont également définis pour chaque donnée.\\
Un exemple de donnée est présente en \autoref{bosphorus_img}

\begin{Figure}{bosphorus_img}{Exemple de données contenues dans la base de données Bosphorus}
 \pgfimage{images/bosphorus.jpg}
\end{Figure}

\subsubsection{Comparatif des différentes bases de données 3D}

Précédemment, je ne vous est présenté que les bases de données les plus populaires. Cependant, beaucoup d'autres existent également.\\
J'ai donc réalisé un tableau (\autoref{tab:bdd3d}) recensant ces différentes bases de données publiquement disponible, et pouvant représenté un intêret dans la reconnaissance d'émotions, avec leurs principales caractéristiques (données principalement tirées de \cite{sota3d}).

\begin{Table}{tab:bdd3d}{Comparaison de bases de données 3D}
	\begin{tabular}{|p{2.5cm}|p{2cm}|p{3.5cm}|p{6cm}|}
		\hline
		\textbf{Nom} &\textbf{Type de données} &\textbf{Taille} &\textbf{Contenu}\\\hline
		BU-3DFE\cite{bu3dfe_article} &Statique &100 adultes &6 émotions basiques avec 4 niveaux d'intensités\\
		BU-4DFE\cite{bu4dfe_article} &Dynamique &101 adultes &6 émotions basiques\\
		Bosphorus\cite{bosphorus_article} &Statique &105 adultes (dont 25 acteurs) &6 émotions basiques, 24 AUs, occlusions\\
		ICT-3DRFE\cite{ict_3drfe} &Statique &23 adultes &6 émotions basiques, 2 expressions neutres, 4 orientations du regard (haut, bas, gauche, droite) et un visage "grimaçant"\\
		D3DFACS\cite{d3dfacs} &Dynamique &10 adultes (dont 4 experts en FACS) &Jusqu'à 38 AUs par sujets\\
		Gavabdb\cite{gavabdb} &Statique &61 adultes &3 expressions: sourires ouverts/fermés et aléatoire\\\hline
	\end{tabular}
\end{Table}


\newpage
\section{2D}
\label{sec:2d}
Passons maintenant à ce qui va m'être utile à la réalisation de mon projet: la 2D.\\
Dans cette section, je présenterai tout d'abord rapidement les dispositifs permettant de récupérer des images en 2D, puis plusieurs bases de données de visages 2D et enfin des méthodes permettant de réaliser de la reconnaissance faciale d'expression.\\

\subsection{Dispositifs d'acquisition d'images 2D}
Contrairement à la 3D, il n'est pas nécessaire d'avoir des dispositifs extrêmement couteux pour acquérir des images en 2D.\\
En effet, un simple appareil photo ou une simple caméra trouvés dans n'importe quelle commerce suffit amplement.\\
Plusieurs entreprises se sont spécialisées dans le domaine de reconnaissance d'émotions et permettent, par exemple, aux publicitaires de tester leur publicités et d'avoir un feedback sur ce que ressentent les spectateurs. Pour cela, ces entreprises(\cite{emotient},\cite{affectiva}) utilisent la webcam intégrée dans les ordinateurs pour ensuite traiter les images.\\
\\
Dans le cadre de ce projet, nous utiliserons une caméra \textbf{PTZ} (Pan, Tilt, Zoom). Ces caméras permettent de faire une rotation selon l'axe Z (Pan), une rotation selon l'axe X (Tilt) et de zoomer selon l'axe Y. Souvent utilisé en temps que caméra de surveillance, dans le cadre de ce projet, ce type de caméra va nous permettre de se déplacer pour trouver la personne présente dans la pièce puis de zoomer sur son visage pour pouvoir ensuite l'analyser.\\
Un exemple de caméra PTZ est présenté en \autoref{ptz}.

\begin{Figure}{ptz}{Exemple de caméra PTZ}
 \pgfimage{images/ptz.png}
\end{Figure}

Dans le cas d'images statiques (images, photos), la reconnaissance se fera directement sur l'image.\\
Par contre dans le cas d'un flux vidéo récupéré via une caméra, c'est les \textit{frames} (images constituant une vidéo) de la vidéo qui vont être analysées.

\newpage
\subsection{Bases de données de visages 2D}
Je vais ici vous présenter différentes bases de données de visages 2D, libre d'accès à la communauté scientifique, intéressantes pour la recherche et permettant de réaliser une reconnaissance faciale d'expressions.

\subsubsection{Cohn-Kanade (CK) \cite{ck}}
Probablement la base de données de visage 2D la plus connue et la plus utilisé pour la reconnaissance faciale d'expressions, elle se compose de 97 sujets (65\% femmes, 15\% afro-américains et 3\% asiatiques ou sud américain) âgés de 18 à 30 ans et réalisant les 6 expressions universelles (joie, tristesse, dégoût, peur, surprise et colère).\\
Cette base contient au total 486 séquences vidéos, du visage neutre au départ jusqu'à l'\textbf{apex} (le pic de l'émotion) à la fin. Ces séquences sont en niveaux de gris et digitalisé en tableaux de 640*480 pixels avec donc une précision de 8 bits (dû aux niveaux de gris).\\
Toutes ces séquences sont entièrement codées en termes de FACS et d'AUs mais ne contiennent pas de label spécifiant l'émotion présentée.\\
Toutes les expressions présentes dans les séquences vidéos contenues dans cette base sont posées et non spontanées, cela signifie qu'on a demandé à ces personnes de produire telle ou telle émotion, cela ne leur est pas venu d'eux-même spontanément. Les expressions présentées seront donc plus "exagérées" qu'en temps normal.

\begin{Figure}{ck}{Exemple de données de CK (de haut en bas et de gauche à droite : neutre, surprise, joie, colère, dégoût)}
 \pgfimage{images/ck.jpg}
\end{Figure}

\subsubsection{CK+ \cite{ckplus}}
C'est la version amélioré de la base de données CK.\\
Cette fois-ci, elle contient des expressions posées et spontanées.\\
%Les expressions spontanées proviennent de \cite{ambadar}. \\
Dans le cas des expressions posées, le nombre de sujets a été augmenté 27\% et le nombre des séquences de 22\%.
Les séquences sont toujours codées en termes de FACS et d'AUS avec, cette fois-ci, un label associé à l'expression présentée qui se trouvera dans les metadata de la vidéos.\\
Cette version propose également des protocoles et des résultats pour le suivi des ponts clés du visage ainsi que pour la reconnaissance d'émotion.\\
\\
Actuellement, une troisième version de CK est en préparation, avec comme principale ajout la synchronisation entre une séquence frontale et une séquence orientée de 30 degrés par rapport à la vue frontal.

\subsubsection{Man-Machine Interaction (MMI)  \cite{mmi1}\cite{mmi2}}
Créée en 2002, cette base de données, comme CK+, se composent de deux parties: une partie posée et une partie spontanée.\\
Cette base contient plus de 2900 vidéos ainsi que des images en haute résolution de 75 sujets (48\% de femmes, européen, africain et sud américain) âgés de 19 à 62 ans. Chaque séquence vidéos contient soit une expression dans son entièreté, soit un AU spécifique\\
Les données de cette base sont également codés en terme de FACS et d'AUs.\\
L'avantage de cette base de données est qu'elle contient toute l'étendu d'une expression: visage neutre - \textbf{onset} (début de l'expression) - \textbf{apex} (pic) - \textbf{offset} (fin de l'expression) - visage neutre .\\
Cela permet de réaliser une reconnaissance d'émotion dans le temps plus précise.\\
Certaines des séquences vidéos sont en couleur, les autres sont en niveaux de gris.

\begin{Figure}{mmi}{Exemple de données de MMI}
 \pgfimage{images/mmi.png}
\end{Figure}

\subsubsection{MHI Mimicry \cite{mimicry1} \cite{mimicry2}}
Cette base de données est intéressantes car elle diffère totalement des deux précédentes au niveau de sa construction.\\
Elle se compose de 54 vidéos (sessions) de 40 sujets (28 hommes, 12 femmes) entre 18 et 40 ans venant de l'Imperial College de Londres. Sur chaque session, 2 participants interagissent et chaque session est divisé en deux parties.\\
Tout d'abord une partie débat dans laquelle les deux participants vont discuter de politique. Ils seront donc soit du même avis, soit d'un avis opposé.\\
La deuxième partie consiste en un "jeu de rôle" : un participant joue le rôle d'un étudiant cherchant un appartement et le deuxième participant joue le rôle d'un propriétaire d'appartement. Le but de l'étudiant est donc de trouver un appartement et celui du propriétaire de louer son appartement, ils ont donc un but qui les relie.\\
Cela permet d'observer le comportement humain lors d'une discussion où l'on est d'accord ou non et lorsque l'on veut convaincre quelqu'un, comment nous "mimons" ou non le comportement de notre interlocuteur dans le but de montrer nos intentions, de nous faire accepter.\\
Chaque session est divisé en "épisodes d’intérêt" et chaque épisode est labellisé en fonction de ce qu'on y voit: sourire, hochement de tête, penchement du corps en avant ou en arrière.\\
Sont aussi labellisés le rôle de chaque participant (s'il parle ou s'il écoute) ainsi que leurs intentions (2 catégories):
\begin{itemize}
\item Social Signal Expression (inconscient) : compréhension, accord, confusion, "liking"
\item Desired Goal (conscient) : flatter l'autre, souligner la compréhension, exprimer l'accord, partager l'empathie, augmenter l'acceptation.\\
\end{itemize}
Tous les enregistrements ont été réalisés avec 15 caméras (7 par participants et une vue d'ensemble) et 3 micros (un micro de tête par participant et un au milieu de la pièce). Un exemple des images récupérées par les caméras se trouvent en \autoref{mimicry}

\begin{Figure}{mimicry}{Exemple de données de MHI Mimicry}
 \pgfimage{images/mimicry.png}
\end{Figure}

Cependant, sur cette base de données, aucune notion de FACS ni d'AUs n'est présente, les données sont labellisées en fonction du ressenti et de l'auto-évaluation réalisée par les participants..

\subsubsection{HCI Tagging \cite{tagging1} \cite{tagging2}}
C'est avec cette base de données que j'ai travaillé à la réalisation de mon projet.\\\\
Créée par la même équipe que celle qui a créée MHI Mimicry, cette base de données se compose de 27 participants (11 hommes, 16 femmes) de 19 à 40 ans.\\
Elle est divisée en 2 parties: la première partie contient 20 vidéos et la deuxième 28 vidéos et 14 images.\\
Chaque vidéos contient la vidéo du visage, l'audio et les expressions vocales, la position où regardent les yeux et les signaux physionomiques (température du corps, rythme de la respiration, EEG, rythme cardiaque). Les vidéos ont été filmées via 6 caméras: 5 en niveaux de gris et d'orientations différentes (frontal, bas-gauche, bas-droite, vue d'ensemble et profil gauche) et une en couleur (frontal).\\
Concernant la partie 1 (Explicit Tagging) : chaque participant regarde plusieurs vidéos et à la fin de chaque vidéo un feedback sur leur ressenti leur ai demandé:
\begin{itemize}
\item émotion ressenti (parmi neutre, anxieux, amusé, triste, joyeux, dégoûté, en colère, surpris ou apeuré)
\item excitation (sur une échelle de 1 à 9)
\item agréabilité (sur une échelle de 1 à 9)
\item dominance (sur une échelle de 1 à 9)
\item prédictibilité (sur une échelle de 1 à 9)
\end{itemize}

Les vidéos visionnées par les participants lors de la partie 1 sont séparées par de petits clips courts neutre pour remettre l'expression de la personne à 0, la réinitialiser en quelque sorte.\\
\\
Concernant la partie 2 (Implicit Tagging) : on diffuse à chaque participant une image ou une vidéo deux fois. La première fois sans rien puis la deuxième fois avec un \textit{tag} censé décrire ce qu'il se passe dans la vidéo/image. Soit ce tag est correct soit il est erroné, le participant doit donc dire si il est d'accord ou non avec le tag attribué en appuyant respectivement sur un bouton vert ou rouge. Pour illustrer cela, un exemple est présent en \autoref{tag}. Cependant, comme les créateurs de cette base n'ont pas les droits pour ces images, les images accessibles que j'ai récupéré ne contiennent que les bords contrairement aux images diffusées aux participants.\\

\begin{Figure}{tag}{Même image avec 2 tags différents: le premier erroné (Kiss), le deuxième correct (Handshake)}
 \pgfimage{images/tag.jpg}
\end{Figure}

Un exemple des différentes données vidéos contenues dans cette base est en \autoref{tag2}.

\begin{Figure}{tag2}{Données contenues dans HCI Tagging}
 \pgfimage{images/tag.png}
\end{Figure}

Pareillement à MHI Mimicry, aucune notion de FACS ni d'AUs n'est présente, les données sont labellisées en fonction de l'auto-évaluation réalisée par tous les participants.

\newpage
\subsubsection{Comparatif des bases de données 2D}
Pour récapituler toutes les informations que j'ai pu donner précédemment et pour introduire de nouvelles bases de données accessibles à la communauté scientifique dont je n'ai pas parlé, j'ai donc réalisé le tableau comparatif suivant:

\begin{Table}{tab:bdd2d}{Comparaison de bases de données 2D}
	\begin{tabular}{|p{1.5cm}|p{0.7cm}|p{4cm}|p{5cm}|p{2.5cm}|}
		\hline
		\textbf{Nom} &\textbf{P/S} &\textbf{Taille} &\textbf{Contenu} &\textbf{Label}\\\hline
		CK\cite{ck} &P & 97 sujets (65\% femmes, 15\% afro-américains et 3\% asiatiques ou sud américain) de 18 à 30 ans &486 séquences vidéos des 6 émotions basiques & FACS\\
		CK+\cite{ckplus} &P/S &Nombre de sujets augmenté 27\% &Nombre de séquences augmenté 27\% &FACS + émotion\\
		MMI\cite{mmi1} \cite{mmi2} &P/S &75 sujets (48\% de femmes, européen, africain et sud américain) de 19 à 62 ans &2900 vidéos des 6 émotions basiques ou d'AUs spécifiques &FACS\\
		MHI Mimicry \cite{mimicry1}\cite{mimicry2} &S & 40 sujets (28 hommes, 12 femmes) entre 18 et 40 ans &54 vidéos (sessions) &Auto-évaluation\\
		HCI Tagging \cite{tagging1}\cite{tagging2} &S &27 sujets (11 hommes, 16 femmes) de 19 à 40 ans &Partie 1 : 20 vidéos; Partie 2 : 28 images et 14 vidéos &Auto-évaluation\\
		SAL \cite{sal} &S &24 sujets &10 heures de vidéo: les sujets parle à une intelligence artificielle et leurs émotions sont changées en fonction des différentes personnalités de l'IA &Feeltrace\\
		JAFFE\cite{jaffe} &P &10 femmes japonaises &213 images des 7 émotions basiques &Noté selon 6 adjectifs d'émotion par 60 sujets Japonais\\
		\hline
	\end{tabular}
\end{Table}

\chapter{Architecture d'un système de reconnaissance d'émotions}
Dans ce chapitre je vais introduire les différentes parties nécessaires à la construction d'un système permettant de reconnaître des expressions et émotions.\\
Un tel système se constitue de 4 parties (ou 3 car la première et deuxième peuvent être combiné en une seule):
\begin{itemize}
\item Détection du visage
\item Extraction des \textit{features} (nez, bouches, yeux …)
\item Classification\\
\end{itemize}

Chacune de ces parties sera décrit plus précisément dans les sections suivantes avec un état de l'art sur les techniques existantes pour chacune d'entre elles.\\
Une partie des informations se trouvant de ce chapitre sont tirées de \cite{sota2d} écrit par les fondateurs de la société Emotient et qui présente un état de l'art des différentes parties d'un système de reconnaissances d'émotions.

\newpage
\section{Détection du visage}
Depuis déjà plusieurs années, la détection de visage dans une image ou vidéo est devenu une réalité grâce aux algorithmes d'apprentissage.\\
La détection de visage sur une vidéo ou une image se retrouve de plus en plus dans notre quotidien. Tout d'abord dans nos appareils photos ou smartphones mais également sur les réseaux sociaux comme Facebook, qui repère les visages sur une photos lorsque l'on souhaite identifier des personnes, ou encore Snapchat, qui depuis la dernière mise à jour repère notre visage grâce à la caméra frontale d'un smartphone pour ensuite lui appliquer diverses animations et déformations.\\
\\
Les algorithmes de détection de visage se divisent en 2 catégories: les absolus et les différentiels, chacun ayant leurs avantages et inconvénients.\\

\subsection{Absolu}
\label{absolu}
Les détecteurs absolus, aussi appelés détecteurs \textit{frame-by-frame}, vont déterminer la position d'un visage sur chaque frame d'une vidéo, indépendamment des frames précédentes.\\
Le principale avantage de ces détecteurs est qu'ils sont très facilement parallélisable sur plusieurs frames d'une vidéo en même temps. Il permet également de réagir très rapidement si jamais le nombre de visages dans l'image change subitement et ne "dérive" pas au court du temps. Ici le terme "dériver" fait référence au fait de perdre la position d'un visage.\\
Cependant, comme dit précédemment, ces détecteurs n'utilisent pas de notions de temps qui pourraient les rendre plus rapide et précis.\\
\\
Le tout premier algorithme de détection qui est maintenant utilisé par la plupart de ces détecteurs est l'algorithme de \textbf{Viola-Jones} \cite{violajones} créé en 2001.\\
Cet algorithme va tout d'abord apprendre un classificateur à différencier des visages d'autres objets grâce à un apprentissage réalisés avec des images de différentes tailles de visages ou de divers autres objets. A noté qu'il est préférable d'avoir un nombre d'objets quelconque très supérieur au nombre de visages si nous voulons que l'algorithme soit efficace.\\
Une fois l'apprentissage fini et que l'on passe une nouvelle image à l'algorithme, il va analyser cette dernière en extrayant plusieurs \textit{patchs}, des bouts de l'image, de différentes tailles qu'il va ensuite normaliser à une taille précise (par exemple 48*48 pixels) puis les donner au classificateur qui va se charger de définir si oui ou non se trouve un visage dans ce patch.\\
Cette étape d'extraction de patchs peut également être paralléliser pour gagner en rapidité.\\

\subsection{Différentiel}
Contrairement aux détecteurs absolus, les détecteurs différentiels, aussi appelés \textit{face trackers}, déterminent la position d'un visage sur une image grâce à sa position précédente. Si la position d'un visage est connu à l'instant \textit{t}, le détecteur différentiel va se servir de cette position pour trouver celle à l'instant \textit{t+1}.\\
Bien sûr, il faut initialiser la position à l'instant 0 pour faire fonctionner ces détecteurs. Pour cela il est possible d'utiliser un détecteur absolu sur la toute première frame du flux vidéo.\\
L'avantage de ces détecteurs est leur grande rapidité et précision. Cependant, l'inconvénient est que l'accumulation de petite erreurs sur les positions peut mener à la "dérive" du détecteur puisqu'il ne se remet jamais à 0 une fois lancé.\\
\\
L'un des plus connus est l'algorithme \textbf{Active Appearance Model} (AAM).\\
Dans AAM, un visage est représenté comme un modèle en forme de maillage triangulaire composé d'environ
70 points. Ce modèle est construit grâce à un apprentissage sur différentes visages réalisant différentes expressions et dans lesquels les \textit{features} sont connus. Il va donc chercher à faire correspondre ce modèle avec tout visage se trouvant sur une image en le déformant pour faire correspondre les features du modèle à celles du visage. Les déformations possibles ont été calculés au préalable.\\
Sur la première frame de la vidéo, il suffit d'initialiser les positions des features de chaque visage (manuellement ou via un tracker absolu) puis, sur les frames suivantes, la position de ces features est trackée grâce aux différentes déformations possibles.\\
\\
AAM ne permet pas que de détecter des visages mais permet également d'en extraire les features.

\subsection{Comparatif}
Voici un comparatif avantages/inconvénients de ces 2 types de détecteurs.

\begin{Table}{tab:comp-detecteur}{Avantages/inconvénients des 2 types de détecteurs}
	\begin{tabular}{|c|p{6cm}|p{6cm}|}
		\hline
		 &\textbf{Avantages} &\textbf{Inconvénients}\\\hline
		\textbf{Absolu} &Facilement parallélisable, très réactif en cas de changement soudain du nombre de visage et ne "dérive" pas &Plus lent et moins précis que les différentiels \\\hline
		\textbf{Différentiel} &Très rapide et d'une grande précision &Peut "dériver" si de petites erreurs s'accumulent au fur et à mesure\\
		\hline
	\end{tabular}
\end{Table}

\newpage
\section{Extraction des \textit{features}}
La deuxième partie d'un système de reconnaissance faciale d'émotions est l'extraction des \textit{features} du visage précédemment repéré. Ces features sont les points clés d'un visage et se composent par exemple de la position du coin des yeux, des lèvres, du nez, des joues, de la position de la pupille, etc. \\
Pour extraire ces points et leurs positions, plusieurs algorithmes existent et je vais maintenant vous en expliquer quelques uns.

\subsection{Filtres de Gabor}
Dans le domaine de la vision par ordinateur, les filtres de Gabor sont principalement utilisés dans l'analyse de textures, la détection de contour et l'extraction d'éléments. C'est un filtre linéaire dont la réponse va être une sinusoïde modulée grâce à une fonction gaussienne.\\
\\
Son fonctionnement est très simple: tout d'abord une fonction de Gabor est générée. Ensuite cette fonction sera appliquée à chaque pixel de l'image. Les résultats obtenus pour chaque pixel seront ensuite stockés et résulteront en la création d'une nouvelle image "filtrée".\\
\\
Une fonction de Gabor peut s'écrire de 3 façons différentes.\\
Tout d'abord, seulement en forme réelle:
\begin{equation}
G(x,y;\lambda ,\theta ,\Psi ,\sigma ,\gamma )=\mathrm{e}(- \frac{x'^2+\gamma ^2 y'^2}{2\sigma ^2})*\mathrm{cos}(2\pi \frac{x'}{\lambda}+\Psi)
\label{gabor_reel}
\end{equation}

Ou seulement en forme imaginaire:
\begin{equation}
G(x,y;\lambda ,\theta ,\Psi ,\sigma ,\gamma )=\mathrm{e}(- \frac{x'^2+\gamma ^2 y'^2}{2\sigma ^2})*\mathrm{sin}(2\pi \frac{x'}{\lambda}+\Psi)
\label{gabor_im}
\end{equation}

Ou bien en combinant \eqref{gabor_reel} et \eqref{gabor_im} pour obtenir la fonction sous forme complexe:
\begin{equation}
G(x,y;\lambda ,\theta ,\Psi ,\sigma ,\gamma )=\mathrm{e}(- \frac{x'^2+\gamma ^2 y'^2}{2\sigma ^2})*\mathrm{e}(\mathrm{i}(2\pi \frac{x'}{\lambda}+\Psi))
\label{gabor_complexe}
\end{equation}
Avec :
\begin{align*}
x'=x\mathrm{cos}\theta + y\mathrm{sin}\theta \\
y'= -x\mathrm{sin}\theta + y\mathrm{cos}\theta
\end{align*}
Et avec comme paramètres:
\begin{itemize}
\item \textit{x} et \textit{y}: les coordonnées du pixel
\item $\lambda $: la longueur d'onde du filtre, en pixel
\item $\theta $: l'orientation du filtre, en degré
\item $\Psi $: le décalage de phase (phase offset), en degré
\item $\sigma $: l'écart type de la gaussienne
\item $\gamma $: le rapport d'aspect spatial (spatial aspect ratio)\\
\end{itemize}

Un dernier paramètre qui peut se révéler utile et ne se trouvant pas directement dans la formule de la fonction est le nombre d'orientation \textit{n}. Ce paramètre va permettre de générer un banc de filtres de différentes orientations en faisant varier le paramètre $\theta $ qui prendra comme valeur $\theta + \frac{i\pi}{n}$ avec i$\in $[0,n-1].\\
Grâce à un site internet permettant de simuler des filtres de Gabor (\cite{simu_gabor})et au mode d'emploi associé (\cite{instr_simu_gabor}), j'ai pu voir l'effet qu'ont les différentes paramètres sur le résultat final.\\
\\
Tout d'abord, pour que mes "expériences" soient cohérentes, il fallait fixer une valeur par défaut pour chaque paramètre ainsi qu'une image de base. Les paramètres, lorsque je ne les modifierai pas, seront donc fixé à: $\lambda = 8 $, $\theta = 0 $, $\Psi = 0 $, $\sigma = 4.48 $ et $\gamma = 0.5 $ et $n = 8 $; quant à l'image de base, ce sera la célèbre photo de Lena (\autoref{lena}).
\begin{Figure}{lena}{Image utilisée pour vérifier l'effet des filtres de Gabor}.
 \pgfimage{images/lena.jpg}
\end{Figure}

J'ai donc fait varier chacun de ces paramètres, un seul à la fois, pour voir l'effet produit sur le résultat. Tous mes résultats, que je vais maintenant expliqué, sont présentés dans les figures de \autoref{lambda} à \autoref{sigma}.\\
\\
J'ai tout d'abord commencé par faire varier $\lambda $ (\autoref{lambda}). Ce paramètre va permettre de faire varier la taille de notre filtre, ce qui va permettre de repérer des bords plus ou moins fins. En consultant le mode d'emploi du simulateur, on apprend que sa valeur doit être supérieur ou égale à 2 et qu'elle doit être plus petit qu'un cinquième de la taille de l'image pour que le filtre fonctionne correctement.\\
\\
Puis j'ai fait varier $\theta $ (\autoref{theta}). En variant l'orientation, cela va permettre au filtre de ne pas capter les même contours: il ne va capter que les contours ayant la même orientation. C'est pour cela que l'on utilise un banc de filtre et non pas un seul filtre tout seul car sinon nous ne capteront que des contours ayant la même orientation (vertical par exemple si $\theta = 0° $).Ses valeurs sont comprises entre 0 et 360.\\
\\
En modifiant $\Psi $ (\autoref{psi}), cela va changer la disposition du filtre, sa "symétrie". Les valeurs sont comprises entre -180 et 180. Les valeurs 0 et 180 correspondent respectivement aux symétries centrés "center-on" et "center-off", et les valeurs -90 et 90 correspondent aux dissymétries associées. Ce paramètre permet de changer "l'emplacement" des contours, comme montré sur la figure associée.\\
\\
Le paramètre $\gamma $ permet de définir l'ellipticité de la fonction de Gabor (\autoref{gamma}). Ses valeurs sont réelles et si $\gamma = 1 $, alors le filtre sera de forme ronde. Plus ce paramètre est élevé, plus le filtre sera "plat". Modifier l'ellipticité permet d'avoir des contours plus ou moins précis.\\
En modifiant le paramètre \textit{n} (\autoref{n}), comme je l'ai expliqué précédemment, cela aura pour conséquence de créer plusieurs filtres d'orientation différentes. Comme un filtre ne permet de trouver que les contours ayant la même orientation, avoir un banc de filtres assez fourni permet de trouver tous les contours d'une image.\\
J'ai enfin terminé par le paramètre $\sigma $ (\autoref{sigma}). Au départ, ce paramètre est inconnu et il se calcule grâce à un nouveau paramètre, \textit{b}, correspondant à la largeur de la bande (en octave). On sait que :
\begin{align*}
b = \log _2 (\frac{\frac{\sigma}{\lambda} \pi + \sqrt{\frac{ln 2}{2}}}{\frac{\sigma}{\lambda} \pi - \sqrt{\frac{ln 2}{2}}})\Leftrightarrow \frac{\sigma}{\lambda} = \frac{1}{\pi } \sqrt{\frac{ln 2}{2}} * \frac{2^b + 1}{2^b - 1}
\end{align*}

En définissant une valeur pour b, on défini donc la valeur de $\sigma $. Lorsque l'on choisit b=1, on obtient $\sigma =0.56\lambda $. Plus b est petit, plus $\sigma $ est grand. Dans mes expériences j'ai fixé b à 1, ce qui m'a donc donné $\sigma = 0.56 * 8 = 4.48$.
\newpage
\begin{Figure}{lambda}{Effet du changement du paramètre $\lambda $}.
 \pgfimage{images/lambda.png}
\end{Figure}
\begin{Figure}{theta}{Effet du changement du paramètre $\theta $}.
 \pgfimage{images/theta.png}
\end{Figure}
\newpage
\begin{Figure}{psi}{Effet du changement du paramètre $\Psi $}.
 \pgfimage{images/psi.png}
\end{Figure}
\begin{Figure}{gamma}{Effet du changement du paramètre $\gamma $}.
 \pgfimage{images/gamma.png}
\end{Figure}
\newpage
\begin{Figure}{n}{Effet du changement du paramètre $n $}.
 \pgfimage{images/n.png}
\end{Figure}
\begin{Figure}{sigma}{Effet du changement du paramètre $b $ associé à $\sigma $}.
 \pgfimage{images/sigma.png}
\end{Figure}

\subsection{Composantes pseudo-Haar}
Les composantes pseudo-Haar ont été crée par Paul Viola et Michael Jones en 2001 lors de la conception de leur détecteur de visage (cf \autoref{absolu} et \cite{violajones}). Cette méthode se base sur les travaux de Papageorgiou et al. (\cite{papa}) qui décrivent des caractéristiques constuites à partir d'ondelettes de Haar (d'où leurs noms de pseudo-Haar).\\
Les composantes utilisées par Viola et Jones sont présentées en \autoref{haar}.

\begin{Figure}{haar}{Composantes pseudo-Haar utilisé par Viola et Jones}.
 \pgfimage{images/haar.png}
\end{Figure}

Ces composantes sont donc des masques, également appelées fenêtres de détection, composées d'un différent nombre de rectangle.\\
Leur utilisation est également très simple. Ces composantes sont appliqués à toutes les positions possibles sur l'image, tout d'abord de petite taille (20*20 pixels par exemple) puis en les agrandissant. A chaque position, on soustrait la somme des pixels contenus dans la partie noire à la somme des pixels contenus dans la partie blanche. Le résultat d'une caractéristique à une certaine position est donc un nombre réel qui va coder les variations du contenu pixellique.\\
\\
Cette méthode est d'une grande rapidité lorsqu'elle est utilisée avec des \textbf{images intégrales}.\\
Une image intégrale est un image dérivé d'une image originale, de la même taille que cette dernière, et dont la valeur de chaque pixel est la somme des pixels au-dessus et des pixels à gauche du pixel courant de l'image originale. Sous forme mathématique, cela se traduit de cette façon:
\begin{align*}
imgInt(x,y)= \sum \nolimits_{x' \leq x , y' \leq y} imgOri(x',y')
\end{align*}

Grâce à ces résultats sous forme de correspondance, il est même possible de calculer la valeur de chaque pixel de façon récursive pour gagner en rapidité de la façon suivante:
\begin{align*}
s(x,y)=s(x,y-1)+imgOri(x,y)\\
imgInt(x,y)=imgInt(x-1,y)+s(x,y)
\end{align*}
avec \textit{s(x,y)} correspondant à la somme cumulée de la ligne \textit{x} jusqu'à la colonne \textit{y}. \\
Une fois l'image intégrale complètement calculée, avoir la somme de n'importe qu'elle rectangle de l'image d'origine s'obtient en seulement 4 accès à l'image intégrale. En prenant un rectangle ABCD, cette somme s'obtient de cette manière:
\begin{align*}
\sum imgOri(ABCD) = imgInt(A) + imgInt(C) - (imgInt(B) + imgInt(D))
\end{align*}
\\
L'année suivante, en 2002, Lienhart et Maydt (\cite{extended_haar})ont créé une extension aux composantes créées par Viola et Jones. Ils ont crées des masques orientés de 45°, ce qui a pour conséquence d'améliorer d'environ 10\% les performances par rapport à l'utilisation seule des masques crées par Viola et Jones. Ces masques permettent désormais de repérer des bords, des lignes ainsi que des "centres", telle que la pupille d'un oeil par exemple. J'ai regroupé ces nouveaux masques dans la \autoref{haar_extended}.

\begin{Figure}{haar_extended}{Extension de composantes pseudo-Haar défini par Lienhart et Maydt}.
 \pgfimage{images/extended_haar.png}
\end{Figure}

Pour aller de paire avec ces nouveaux masques et l'utilisation des images intégrales, une nouvelle formule est nécessaire qui calcule maintenant la somme dans un demi-rectangle orienté de 45° :
\begin{align*}
imgInt(x,y)= \sum \nolimits_{x' \leq x , x' \leq x - |y-y'|} imgOri(x',y')
\end{align*}


\subsection{Local Binary Pattern (LBP)}

\newpage
\section{Classification}
Une fois les features extraites, il ne reste plus qu'à les analyser pour les classifier.\\
Deux types de classificateurs existent: les systèmes basés sur des règles d'experts (\textit{rule-based expert systems}) et les classificateurs avec apprentissage (\textit{machine learning classifiers}).
Les premiers sont de moins en moins répandu car ils sont beaucoup plus compliqué à utiliser que les autres. C'est donc sur les classificateurs avec apprentissage que je vais me concentrer.\\
Parmi ces classificateurs, nous pouvons les classer en 2 catégories: les binaires et les multi-classes.

\subsection{Classificateurs binaires}
Comme l'indique son nom, ce type de classificateur ne permet de classifier une instance qu'entre 2 classes (ex: sourire VS pas de sourire).\\
\\
Le classificateur binaire le plus connu est le \textbf{Support Vector Machine} (SVM).\\
Les SVMs se basent sur 2 notions très importantes.\\
La première est la notion de \textit{marge maximale}. Cette marge est la distance entre la frontière de séparation des 2 classes et les échantillons les plus proches appelés vecteurs supports. La frontière de séparation est choisi de telle sorte qu'elle maximise le plus la marge. C'est donc grâce à un apprentissage que l'on peut définir cette frontière.\\
La deuxième notion est celle de la transformation de l'espace de représentation en un espace de dimension plus grand, voir infini. cette notion est utile lorsque les données ne sont pas linéairement séparable dans l'espace d'origine. En projetant dans un espace de dimension plus grand, il est plus probable de trouver une séparation \autoref{svm}.\\
Cette transformation est réalisé via une fonction noyau qui va permettre de transformer un produit scalaire dans un espace de grande dimension.

\begin{Figure}{svm}{Exemple de projection dans un espace plus grand réalisé par SVM}
 \pgfimage{images/svm.png}
\end{Figure}


\subsection{Classificateurs multi-classes}
Contrairement aux binaires, ces classificateurs permettent de classifier une instance selon un nombre de classes supérieur à 2.\\
\\
Le classificateur multi-classes le plus connu est le \textbf{k-Plus Proche Voisin} (kPPV).\\
Le kPPV fonctionne d'une manière très simple.\\
Tout d'abord un apprentissage grâce à une base d'apprentissage composé de couples "entrée-sortie".\\
Puis la phase de classification se fait en cherchant la distance minimum entre un nouvel échantillon d'entrée dont la sortie doit être déterminée et les \textit{k} échantillons d'apprentissage dont l'entrée est la plus proche.\\
Une fois ces k plus proche voisins trouvés, il suffit de trouver quelle classe est majoritairement présente pour trouver la sortie associée et donc à quelle classe appartient le nouvel échantillon. Si jamais il y a un nombre égale d'échantillon de plusieurs classes (par exemple avec k=3 on obtient 1 échantillons de 3 différentes classes), on choisi alors aléatoirement la classe d'appartenance.\\
Un exemple vous est présenté en \autoref{kppv} dans lequel 3 classes ont été appris via la phase d'apprentissage et où k est fixé à 3; quatre échantillons doivent être classés dont un se trouvant en zone d'indécision.

\begin{Figure}{kppv}{Fonctionnement du kPPV avec k=3, 3 classes et 4 échantillons dont un indécis}
 \pgfimage{images/kppv.png}
\end{Figure}


\chapter{Critiques de l'existant et nouvelle approche}
\label{chap:chap_critiques}

\chapter{Spécifications de l'application}
\label{chap:chap_specif}


\chapter{Planning et outils utilisés}
\label{chap:chap_planning}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Développement}
\label{part:part_dev}


\unnumberedchapter{Conclusion}

\appendix

\weeklyreport{17/09/2015}{
	Découverte de 2 grandes méthodes de description des mouvements du visage existent : le FACS (Facial Action Coding System) mis en place par P. Ekman et w. Friesen en 1978 et le FAPU (Facial Animation Parameter Units) introduit par la norme de codage vidéo MPEG-4.\\
	Recherche sur les types d'acquisitions d'images en 2D ou en 3D avec le matériel nécessaires à chaque fois ainsi que les algorithmes disponibles.
}
\weeklyreport{24/09/2015}{
	Recherche sur comment se décompose un bon système de reconnaissance facial d'émotions.\\
	Décomposition en 4 parties (récupération du visage, normalisation, extraction des points clés, classification) et recherche plus poussée sur les 2 premières parties.
}
\weeklyreport{01/10/2015}{
	Continuation des recherches sur les 2 dernières parties du système.\\
	Recherche également sur les différentes bases de données 2D publics disponibles à l'utilisation.
}
\weeklyreport{08/10/2015}{
	Recherche plus approfondies sur les filtres de Gabor. Présentation de mes recherches à Messieurs Conte et Slimane.\\
	Commencement de l'écriture du rapport.
}
\weeklyreport{15/10/2015}{
	Continuation des recherches sur les filtres de Gabor et leur fonctionnement. J'ai essayé de comprendre le fonctionnement des filtres et l'impact des différents paramètres. Grâce à un simulateur que j'ai trouvé en ligne (\cite{simu_gabor}) et aux instructions associées (\cite{instr_simu_gabor}), j'ai pu constater l'effet qu'ont les différents paramètres sur le résultat final.
}
\weeklyreport{22/10/2015}{
	Étude approfondi des bases de données MMI Mimicry et HCI Tagging.
}
\weeklyreport{05/11/2015}{
	Documentation sur les caractéristiques pseudo-Haar et leur fonctionnement.\\
	Recherche de techniques permettant de placer les points clés d'un visage sans FACS (ASM, AAM ...)
}
\weeklyreport{12/11/2015}{
	Réunion avec Mrs Conte et Slimane : décision de l'arrêt de la phase état de l'art pour commencer le développement; prise de décision sur les spécifications de notre système.	
}
\weeklyreport{19/11/2015}{
	Documentation plus poussée sur ASM, récupération d'un programme Matlab d'analyse d'émotions réalisé par des collègues italiens à Mr Conte et tentative de le faire fonctionner sous Octave vu que nous ne possédons pas de licence Matlab.\\
	Commencement de la prise en main de la librairie C++ OpenCV mais suite à un entretien avec Mr Conte, la décision a été prise de changer les spécifications de notre programme pour continuer le travail qui a déjà été réalisés par ses collègues italiens.
}
\weeklyreport{26/11/2015}{
	Étude approfondi de l'article écrit par Vitale et al. (\cite{italiens}) et rendez vous avec Mr Conte pour faire fonctionner le programme Matlab, presque fonctionnel au final.
}
\weeklyreport{03/12/2015}{
	Fin de l'étude approfondi de l'article de Vitale et al.\\
	Travail sur le programme Matlab pour le faire fonctionner à 100\%.
}
\weeklyreport{10/12/2015}{
	Rédaction du rapport.\\
	Téléchargement de Matlab pour faire fonctionner le programme car il est impossible de le faire fonctionner avec Octave.\\
	RDV avec Mr Conte pour vérifier la compréhension de l'article, des zones d'ombre persistent.
}
\weeklyreport{17/12/2015}{
	Étude du programme Matlab fonctionnel et comparaison de l'article au programme pour trouver à quelle partie du programme correspond chaque partie de l'article dans le but de mieux le comprendre.
}
\weeklyreport{Vacances de Noël}{
	
}
\weeklyreport{07/01/2016}{
	
}
\weeklyreport{14/01/2016}{
	
}
\weeklyreport{21/01/2016}{
	
}

% petite astuce pour tout citer : A NE PAS REPRODUIRE DANS VOTRE RAPPORT
\nocite{*}


\makelastpages

\end{document}


